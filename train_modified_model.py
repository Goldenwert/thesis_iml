import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments
from copy import deepcopy
import os
import time
from transformers import GPT2Tokenizer
from torch.utils.checkpoint import checkpoint

class MultiTokenGPT2(nn.Module):
    #gpt2 with shared trunk and multiple prediction heads for parallel token generation
    def __init__(self, base_model_name="gpt2", num_tokens_to_predict=4, trunk_layers=None, use_gradient_checkpointing=True):
        super().__init__()
        self.num_tokens_to_predict = num_tokens_to_predict
        self.use_gradient_checkpointing = use_gradient_checkpointing

        self.config = GPT2Config.from_pretrained(base_model_name)
        self.config.use_cache = False

        #load pretrained model for layer extraction
        full_model = GPT2LMHeadModel.from_pretrained(base_model_name)
        original_layers = full_model.config.n_layer
        
        #calculate trunk layers
        if trunk_layers is None:
            trunk_layers = original_layers - (num_tokens_to_predict - 1)
        
        print(f"ðŸ”§ Multi-token GPT2 Architecture:")
        print(f"   - Trunk layers: {trunk_layers}")
        print(f"   - Prediction heads: {num_tokens_to_predict}")
        print(f"   - Gradient checkpointing: {use_gradient_checkpointing}")
        
        #create shared trunk
        self.shared_trunk = nn.Module()
        self.shared_trunk.wpe = deepcopy(full_model.transformer.wpe)
        self.shared_trunk.wte = deepcopy(full_model.transformer.wte)
        self.shared_trunk.drop = deepcopy(full_model.transformer.drop)
        
        #trunk layers
        self.shared_trunk.h = nn.ModuleList(deepcopy(full_model.transformer.h[:trunk_layers]))
        
        #setup trunk config
        self.config.n_layer = trunk_layers
        self.shared_trunk.config = deepcopy(self.config)

        hidden_size = self.config.n_embd
        vocab_size = self.config.vocab_size
        
        #create prediction heads
        self.first_head = deepcopy(full_model.transformer.h[trunk_layers])
        
        #extra heads from remaining layers
        self.extra_heads = nn.ModuleList()
        for i in range(1, num_tokens_to_predict):
            layer_idx = trunk_layers + i
            if layer_idx < original_layers:
                self.extra_heads.append(deepcopy(full_model.transformer.h[layer_idx]))
            else:
                self.extra_heads.append(deepcopy(full_model.transformer.h[-1]))

        #single norm and output projection for all heads
        self.norm = deepcopy(full_model.transformer.ln_f)
        self.output = nn.Linear(hidden_size, vocab_size, bias=False)
        self.output.weight = nn.Parameter(full_model.lm_head.weight.clone())
        
        #tie embeddings to output
        self.shared_trunk.wte.weight = self.output.weight

        del full_model
        print(f"âœ… Architecture ready: shared norm + projection for stacked heads")
    
    def _head_forward(self, head, trunk_output, attention_mask):
        #single head forward for gradient checkpointing
        head_outputs = head(trunk_output, attention_mask=attention_mask)
        return head_outputs[0]
    
    def forward(self, input_ids, attention_mask=None, labels=None, return_all_heads=False):
        #forward pass with multi-token prediction
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        
        batch_size, seq_length = input_ids.size()
        
        #token and position embeddings
        inputs_embeds = self.shared_trunk.wte(input_ids)
        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        position_embeds = self.shared_trunk.wpe(position_ids)
        
        #combine embeddings
        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.shared_trunk.drop(hidden_states)
        
        #prepare attention mask
        if attention_mask is not None:
            attention_mask = attention_mask.view(batch_size, -1)
            attention_mask = attention_mask[:, None, None, :]
            attention_mask = attention_mask.to(dtype=hidden_states.dtype)
            attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_states.dtype).min

        #pass through trunk layers
        for block in self.shared_trunk.h:
            outputs = block(hidden_states, attention_mask=attention_mask)
            hidden_states = outputs[0]
        
        h_trunk = hidden_states
        
        # EXACT META'S APPROACH: Process through prediction heads
        prediction_heads = [self.first_head] + list(self.extra_heads)
        n_heads_to_use = self.num_tokens_to_predict if return_all_heads else 1
        
        latents = []
        for i in range(n_heads_to_use):
            if self.use_gradient_checkpointing and self.training:
                # Use gradient checkpointing for memory efficiency
                head_hidden = checkpoint(self._head_forward, prediction_heads[i], h_trunk, attention_mask)
            else:
                # Standard forward pass
                head_outputs = prediction_heads[i](h_trunk, attention_mask=attention_mask)
                head_hidden = head_outputs[0]
            
            latents.append(head_hidden)
        
        # EXACT META APPROACH: Stack first, then apply norm once, then project once
        if n_heads_to_use == 1:
            # Single head case - still follow Meta's approach
            h = latents[0].unsqueeze(-2)  # Add head dimension: [batch, seq, 1, dim]
            h = self.norm(h)              # Apply norm
            logits = self.output(h).float()  # Apply projection
            logits = logits.squeeze(-2)   # Remove head dimension: [batch, seq, vocab]
            
            # Calculate loss if labels provided (standard next-token prediction)
            loss = None
            if labels is not None:
                # Shift so that tokens < n predict n
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()
                
                # Calculate cross entropy loss
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            
            return {
                "loss": loss,
                "logits": logits,
                "hidden_states": h_trunk,
            }
        else:
            # EXACT META APPROACH: Multi-head case
            # Stack all heads: [batch, seq, n_heads, dim]
            h = torch.stack(latents, dim=-2)
            
            # Apply norm to stacked tensor (Meta's approach)
            h = self.norm(h)
            
            # Apply output projection to stacked tensor (Meta's approach)
            output = self.output(h).float()  # [batch, seq, n_heads, vocab]
            
            # SIMPLIFIED LOSS CALCULATION (following standard practice)
            loss = None
            if labels is not None:
                total_loss = 0.0
                valid_heads = 0
                
                for head_idx in range(n_heads_to_use):
                    # Head i predicts token at position t+i+1 given tokens up to position t
                    head_logits = output[:, :, head_idx, :]  # [batch, seq, vocab]
                    
                    # Shift: logits at position t predict labels at position t+head_idx+1
                    if head_idx == 0:
                        # Standard next-token prediction: predict position t+1 from position t
                        shift_logits = head_logits[:, :-1, :].contiguous()
                        shift_labels = labels[:, 1:].contiguous()
                    else:
                        # Multi-token prediction: predict position t+head_idx+1 from position t
                        if seq_length > head_idx + 1:
                            shift_logits = head_logits[:, :-(head_idx+1), :].contiguous()
                            shift_labels = labels[:, head_idx+1:].contiguous()
                        else:
                            continue
                    
                    # Ensure same length
                    min_len = min(shift_logits.size(1), shift_labels.size(1))
                    shift_logits = shift_logits[:, :min_len, :]
                    shift_labels = shift_labels[:, :min_len]
                    
                    if shift_labels.numel() > 0:
                        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                        head_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                                           shift_labels.view(-1))
                        total_loss += head_loss
                        valid_heads += 1
                
                if valid_heads > 0:
                    loss = total_loss / valid_heads
                else:
                    loss = torch.tensor(0.0, requires_grad=True, device=hidden_states.device)
            
            # Return first head logits for compatibility, full logits for analysis
            return {
                "loss": loss,
                "logits": output[:, :, 0, :],  # First head for trainer compatibility
                "all_head_logits": output,     # Full multi-head output [batch, seq, n_heads, vocab]
                "hidden_states": h_trunk,
            }
    
    def enhanced_speculative_generate(self, input_ids, attention_mask=None, max_new_tokens=100, 
                                    temperature=0.0, top_k=0, top_p=1.0, do_sample=False):
        """
        ENHANCED speculative decoding that accepts multiple tokens per cycle.
        This implements the 3x throughput improvement mentioned in the paper.
        """
        device = next(self.parameters()).device
        
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids).to(device)

        generated = input_ids.clone()
        current_attention_mask = attention_mask.clone()
        
        num_tokens_generated = 0
        total_tokens_generated = 0
        total_accepted_tokens = 0
        acceptance_rates = []
        
        print(f"ðŸš€ Enhanced Speculative Decoding (Multi-token acceptance)")
        
        while num_tokens_generated < max_new_tokens:
            # Generate candidate tokens from all heads
            outputs = self.forward(
                generated,
                attention_mask=current_attention_mask,
                return_all_heads=True
            )
            
            all_head_logits = outputs["all_head_logits"]  # [batch, seq, n_heads, vocab]
            
            # Extract candidate tokens from each head
            candidates = []
            for head_idx in range(self.num_tokens_to_predict):
                head_logits = all_head_logits[0, -1, head_idx, :]  # Last position, specific head
                
                if do_sample and temperature > 0:
                    if top_k > 0:
                        head_logits = self._top_k_filtering(head_logits, top_k)
                    if top_p < 1.0:
                        head_logits = self._top_p_filtering(head_logits, top_p)

                    probs = torch.softmax(head_logits / temperature, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(head_logits, dim=-1)
                
                candidates.append(next_token.item())
            
            # Create candidate sequence with all predicted tokens
            candidate_tokens = torch.tensor(candidates, device=device).unsqueeze(0)
            candidate_sequence = torch.cat([generated, candidate_tokens], dim=1)
            candidate_mask = torch.cat([
                current_attention_mask, 
                torch.ones(1, len(candidates), device=device)
            ], dim=1)
            
            # VERIFY all candidates using the first head (backbone verification)
            with torch.no_grad():
                verify_outputs = self.forward(
                    candidate_sequence[:, :-1],  # All tokens except the last
                    attention_mask=candidate_mask[:, :-1],
                    return_all_heads=False  # Use single head for verification
                )
            
            verify_logits = verify_outputs["logits"][0, -len(candidates):, :]  # Last n positions
            
            # Check how many candidates are accepted (longest valid prefix)
            accepted_count = 0
            for i, candidate_token in enumerate(candidates):
                if i >= verify_logits.size(0):
                    break
                    
                predicted_token = torch.argmax(verify_logits[i], dim=-1).item()
                
                if predicted_token == candidate_token:
                    accepted_count += 1
                else:
                    break  # Stop at first mismatch
            
            # Accept the longest valid prefix (at least 1 token)
            if accepted_count == 0:
                accepted_count = 1  # Always accept at least the first token
            
            # Update generated sequence
            accepted_tokens = candidate_tokens[:, :accepted_count]
            generated = torch.cat([generated, accepted_tokens], dim=1)
            current_attention_mask = torch.cat([
                current_attention_mask,
                torch.ones(1, accepted_count, device=device)
            ], dim=1)
            
            # Update counters
            num_tokens_generated += accepted_count
            total_tokens_generated += 1  # One generation cycle
            total_accepted_tokens += accepted_count
            acceptance_rates.append(accepted_count / len(candidates))
            
            # Stop if we've generated enough tokens
            if num_tokens_generated >= max_new_tokens:
                break
        
        # Calculate and report speedup
        if total_tokens_generated > 0:
            avg_acceptance_rate = sum(acceptance_rates) / len(acceptance_rates)
            effective_speedup = total_accepted_tokens / total_tokens_generated
            
            print(f"âœ… Speculative Decoding Results:")
            print(f"   - Tokens generated: {total_accepted_tokens}")
            print(f"   - Generation cycles: {total_tokens_generated}")
            print(f"   - Average acceptance rate: {avg_acceptance_rate:.2%}")
            print(f"   - Effective speedup: {effective_speedup:.2f}x")
            print(f"   - Target speedup: 3.0x (paper's result)")

        return generated
    
    def generate(self, input_ids, attention_mask=None, max_new_tokens=100, temperature=0.0, 
                top_k=0, top_p=1.0, do_sample=False, use_speculative=True):
        """
        Generate text using either standard autoregressive generation or enhanced speculative decoding.
        """
        if use_speculative and self.num_tokens_to_predict > 1:
            return self.enhanced_speculative_generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=do_sample
            )
        else:
            # Fall back to standard generation
            return self._standard_generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=do_sample
            )
    
    def _standard_generate(self, input_ids, attention_mask=None, max_new_tokens=100, 
                          temperature=0.0, top_k=0, top_p=1.0, do_sample=False):
        """Standard autoregressive generation (one token at a time)."""
        device = next(self.parameters()).device
        
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids).to(device)

        generated = input_ids.clone()
        current_attention_mask = attention_mask.clone()

        for _ in range(max_new_tokens):
            outputs = self.forward(
                generated,
                attention_mask=current_attention_mask,
                return_all_heads=False
            )

            next_token_logits = outputs["logits"][:, -1, :]

            if do_sample and temperature > 0:
                if top_k > 0:
                    next_token_logits = self._top_k_filtering(next_token_logits, top_k)
                if top_p < 1.0:
                    next_token_logits = self._top_p_filtering(next_token_logits, top_p)

                probs = torch.softmax(next_token_logits / temperature, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            generated = torch.cat([generated, next_token], dim=1)
            current_attention_mask = torch.cat([
                current_attention_mask, 
                torch.ones(generated.size(0), 1, device=device)
            ], dim=1)

        return generated
    
    def _top_p_filtering(self, logits, top_p):
        """Apply top-p (nucleus) sampling to logits tensor."""
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
        
        # Remove tokens with cumulative probability above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        # Shift the indices to the right to keep the first token above threshold
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # Scatter sorted tensors to original indexing
        indices_to_remove = sorted_indices_to_remove.scatter(
            -1, sorted_indices, sorted_indices_to_remove
        )
        filtered_logits = logits.masked_fill(indices_to_remove, -float('Inf'))
        return filtered_logits

    def _top_k_filtering(self, logits, top_k):
        """Apply top-k filtering to logits tensor."""
        if top_k <= 0:
            return logits
            
        values, indices = torch.topk(logits, k=top_k, dim=-1)
        filtered_logits = torch.full_like(logits, -float('Inf'))
        filtered_logits = filtered_logits.scatter(-1, indices, values)
        return filtered_logits


def train_multi_token_gpt2(
    train_dataloader,
    val_dataloader,
    tokenizer,
    output_dir: str = "multi_gpt2_outputs",
    num_tokens_to_predict: int = 4,
    num_train_epochs: int = 1,
    learning_rate: float = 5e-5,
    gradient_accumulation_steps: int = 8
):
    """
    Train the MultiTokenGPT2 model with improved architecture.
    
    This implementation follows the approach described in the paper
    'Better & Faster Large Language Models via Multi-token Prediction'.
    """
    from transformers import GPT2Config, get_scheduler
    config = GPT2Config.from_pretrained("gpt2")

    trunk_layers = config.n_layer - (num_tokens_to_predict - 1)  # 12 - 3 = 9 for n=4
    
    print(f"ðŸ”§ FIXED MTP Model - Following Original Meta Implementation:")
    print(f"   âœ… Trunk layers: {trunk_layers} (removed {num_tokens_to_predict - 1} layers)")  
    print(f"   âœ… Prediction heads: {num_tokens_to_predict} (1 main + {num_tokens_to_predict-1} extra)")
    print(f"   âœ… Architecture: Shared trunk â†’ Multiple heads â†’ Shared unembedding")
    print(f"   âœ… Loss: Simplified multi-token loss (no complex alignment logic)")
    print(f"   âœ… Training: Standard transformer training (no head weighting)")
    print(f"ðŸŽ¯ Expected: Much lower loss than previous implementation!")
    
    model = MultiTokenGPT2(
        base_model_name="gpt2", 
        num_tokens_to_predict=num_tokens_to_predict,
        trunk_layers=trunk_layers
    )
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    training_args_dict = {
        "output_dir": output_dir,
        "num_train_epochs": num_train_epochs,
        "learning_rate": learning_rate,
        "per_device_train_batch_size": 4,
        "per_device_eval_batch_size": 4,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "logging_dir": f"{output_dir}/logs",
        "logging_steps": 1000,
        "remove_unused_columns": False,
        "warmup_steps": 500,
        "weight_decay": 0.01,
        "load_best_model_at_end": False,
        "dataloader_num_workers": 0,
        "fp16": torch.cuda.is_available(),
        "fp16_opt_level": "O1",
        "report_to": "none",
        "save_safetensors": False,
        "evaluation_strategy": "no",
        "save_strategy": "epoch",
        "save_steps": 99999999,
        "save_total_limit": 1,
    }

    try:
        if hasattr(train_dataloader, 'batch_size') and train_dataloader.batch_size is not None:
            training_args_dict["per_device_train_batch_size"] = train_dataloader.batch_size
        
        if val_dataloader and hasattr(val_dataloader, 'batch_size') and val_dataloader.batch_size is not None:
            training_args_dict["per_device_eval_batch_size"] = val_dataloader.batch_size
    except (AttributeError, TypeError):
        print("Warning: Could not detect batch sizes from dataloaders. Using default values.")

    try:
        training_args = TrainingArguments(**training_args_dict)
    except TypeError as e:
        print(f"Warning: Some training arguments not supported: {e}")
        basic_args = {
            "output_dir": output_dir,
            "num_train_epochs": num_train_epochs,
            "learning_rate": learning_rate,
            "per_device_train_batch_size": train_dataloader.batch_size,
            "per_device_eval_batch_size": val_dataloader.batch_size if val_dataloader else 4,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "logging_dir": f"{output_dir}/logs",
            "save_steps": 99999999,
            "report_to": "none",
            "save_safetensors": False,
        }
        training_args = TrainingArguments(**basic_args)

    class MultiTokenTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            """
            SIMPLIFIED LOSS COMPUTATION following Meta's approach.
            The model's forward method handles all the multi-token prediction logic.
            """
            # Clean inputs - only keep what the model needs
            filtered_inputs = {
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs.get("attention_mask"),
                "labels": inputs.get("labels"),
                "return_all_heads": True  # Use all heads during training
            }
            
            # Forward pass through model
            outputs = model(**filtered_inputs)
            
            # Extract loss - the model computes it internally
            loss = outputs.get("loss")
            
            if loss is None:
                # Fallback: compute standard next-token loss
                logits = outputs["logits"]
                labels = filtered_inputs["labels"]
                
                if labels is not None:
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                                   shift_labels.view(-1))
                else:
                    loss = torch.tensor(0.0, requires_grad=True, device=logits.device)
            
            if return_outputs:
                return loss, outputs
            
            return loss
            
        # REMOVED: Let the default Trainer.training_step handle everything
        # The issue was overriding training_step incorrectly which broke gradient flow
            
        def _save_checkpoint(self, model, trial, *args, **kwargs):
            """Override to handle DTensor import issues"""
            try:
                # Call the parent method with all arguments passed through
                super()._save_checkpoint(model, trial, *args, **kwargs)
            except ImportError as e:
                if "DTensor" in str(e):
                    print(f"Warning: Skipping checkpoint save due to DTensor compatibility issue: {e}")
                    print("Model will still be saved at the end of training.")
                else:
                    raise e
            except TypeError as e:
                # Handle signature mismatch - try different argument combinations
                print(f"Warning: Checkpoint save signature mismatch, trying different approaches: {e}")
                try:
                    # Try without extra args
                    super()._save_checkpoint(model, trial)
                except ImportError as e2:
                    if "DTensor" in str(e2):
                        print(f"Warning: Skipping checkpoint save due to DTensor compatibility issue: {e2}")
                        print("Model will still be saved at the end of training.")
                    else:
                        raise e2
                except Exception as e3:
                    print(f"Warning: Could not save checkpoint: {e3}")
                    print("Model will still be saved at the end of training.")
        
        def save_model(self, output_dir=None, _internal_call=False):
            """Override to handle DTensor import issues during saving"""
            if output_dir is None:
                output_dir = self.args.output_dir
                
            try:
                super().save_model(output_dir, _internal_call)
            except ImportError as e:
                if "DTensor" in str(e):
                    print(f"Warning: Using fallback save method due to DTensor compatibility issue")
                    # Fallback: manually save model weights
                    os.makedirs(output_dir, exist_ok=True)
                    torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
                    print(f"Model saved to {output_dir} (fallback method)")
                else:
                    raise e
            
        def get_batch_samples(self, epoch_iterator, num_batches, device=None):
            """
            Override the get_batch_samples method to handle missing fields in batch.
            This makes the code more robust against KeyErrors.
            """
            batch_samples = []
            total_items = 0
            
            for _ in range(num_batches):
                try:
                    # Get the next batch
                    batch = next(epoch_iterator)
                    
                    # We only need input_ids, attention_mask, and labels for the model
                    # Filter out any other keys that might cause issues
                    filtered_batch = {}
                    for k in ["input_ids", "attention_mask", "labels"]:
                        if k in batch:
                            filtered_batch[k] = batch[k]
                    
                    # Add to our batch samples
                    batch_samples.append(filtered_batch)
                    
                    # Count the number of items
                    if "input_ids" in filtered_batch:
                        total_items += filtered_batch["input_ids"].size(0)
                    
                except StopIteration:
                    break
                
            return batch_samples, total_items

        def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=None):
            """Override to handle DTensor import issues during evaluation"""
            try:
                super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)
            except ImportError as e:
                if "DTensor" in str(e):
                    print(f"Warning: Skipping save/evaluate due to DTensor compatibility issue: {e}")
                else:
                    raise e

        def _save(self, output_dir, state_dict=None):
            """Override _save to avoid DTensor NameError issues"""
            try:
                super()._save(output_dir, state_dict)
            except NameError as e:
                if "DTensor" in str(e):
                    print(f"Warning: DTensor NameError in _save, using manual save approach")
                    # Manual save approach to avoid DTensor issues
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # Save model weights
                    if state_dict is None:
                        state_dict = self.model.state_dict()
                    torch.save(state_dict, os.path.join(output_dir, "pytorch_model.bin"))
                    
                    # Save config
                    if hasattr(self.model, 'config'):
                        self.model.config.save_pretrained(output_dir)
                    
                    # Save tokenizer if available
                    if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                        try:
                            self.tokenizer.save_pretrained(output_dir)
                        except:
                            pass
                    elif hasattr(self, 'processing_class') and self.processing_class is not None:
                        try:
                            self.processing_class.save_pretrained(output_dir)
                        except:
                            pass
                    
                    print(f"Model manually saved to {output_dir}")
                else:
                    raise e
            except ImportError as e:
                if "DTensor" in str(e):
                    print(f"Warning: DTensor ImportError in _save, using manual save approach")
                    # Same fallback as above
                    os.makedirs(output_dir, exist_ok=True)
                    
                    if state_dict is None:
                        state_dict = self.model.state_dict()
                    torch.save(state_dict, os.path.join(output_dir, "pytorch_model.bin"))
                    
                    if hasattr(self.model, 'config'):
                        self.model.config.save_pretrained(output_dir)
                    
                    if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                        try:
                            self.tokenizer.save_pretrained(output_dir)
                        except:
                            pass
                    elif hasattr(self, 'processing_class') and self.processing_class is not None:
                        try:
                            self.processing_class.save_pretrained(output_dir)
                        except:
                            pass
                    
                    print(f"Model manually saved to {output_dir}")
                else:
                    raise e

    # Fix for deprecated tokenizer parameter
    trainer_kwargs = {
        "model": model,
        "args": training_args,
        "train_dataset": train_dataloader.dataset,
        "eval_dataset": val_dataloader.dataset if val_dataloader else None,
        "data_collator": train_dataloader.collate_fn
    }
    
    # Use processing_class for newer versions, tokenizer for older versions
    try:
        trainer_kwargs["processing_class"] = tokenizer
        trainer = MultiTokenTrainer(**trainer_kwargs)
    except TypeError:
        # Fallback to tokenizer for older versions
        trainer_kwargs["tokenizer"] = tokenizer
        trainer = MultiTokenTrainer(**trainer_kwargs)

    trainer.train()

    # Save model with error handling
    try:
        trainer.save_model(output_dir)
    except ImportError as e:
        if "DTensor" in str(e):
            print(f"Warning: Using fallback save method due to DTensor compatibility issue")
            # Fallback: manually save model weights
            os.makedirs(output_dir, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
            print(f"Model saved to {output_dir} (fallback method)")
        else:
            raise e

    return model

def test_blockwise_speculative_decoding():
    """Test function for the new blockwise speculative decoding"""
    print("Testing Blockwise Speculative Decoding...")
    
    # Load model and tokenizer
    model_name = "gpt2"
    model = MultiTokenGPT2(model_name, num_tokens_to_predict=4)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Test prompt
    prompt = "The future of artificial intelligence is"
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    print(f"Prompt: {prompt}")
    print("Generating with blockwise speculative decoding...")
    
    # Generate with speculative decoding
    start_time = time.time()
    generated_ids = model.generate(
        input_ids=input_ids,
        max_new_tokens=50,
        use_speculative=True,
        do_sample=True,
        temperature=0.8,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id
    )
    generation_time = time.time() - start_time
    
    # Decode result
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(f"Generated: {generated_text}")
    print(f"Generation time: {generation_time:.2f} seconds")
    
    return generated_text

if __name__ == "__main__":
    test_blockwise_speculative_decoding()
