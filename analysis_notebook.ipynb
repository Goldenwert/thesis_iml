{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Token Prediction and MEDUSA: Accelerating Large Language Models\n",
    "\n",
    "This notebook demonstrates the implementation of two advanced techniques for accelerating language model inference:\n",
    "\n",
    "1. **Multi-Token Prediction (MTP)** based on the paper \"Better & Faster Large Language Models via Multi-token Prediction\"\n",
    "2. **MEDUSA** framework based on the paper \"MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\"\n",
    "\n",
    "Both techniques modify the standard next-token prediction task to predict multiple future tokens at once, using different architectural approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib tqdm datasets transformers[torch] accelerate>=0.26.0\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "We'll use the MetamathQA dataset, which includes mathematical and computational reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import preprocess_data, explore_dataset, validate_tokenization\n",
    "explore_dataset(\"meta-math/MetaMathQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "split_ratio = 0.1\n",
    "seed = 42\n",
    "max_examples = 500\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer = preprocess_data(\n",
    "    model_name=\"gpt2\",\n",
    "    max_seq_len=max_seq_len,\n",
    "    train_batch_size=train_batch_size,\n",
    "    val_batch_size=val_batch_size,\n",
    "    split_ratio=split_ratio,\n",
    "    seed=seed,\n",
    "    max_examples=max_examples,\n",
    "    num_tokens_to_predict=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "We'll implement and compare three different model architectures:\n",
    "1. Standard Next-Token Prediction (NTP)\n",
    "2. Multi-Token Prediction (MTP)\n",
    "3. MEDUSA\n",
    "\n",
    "### 3.1 Standard Next-Token Prediction Model\n",
    "\n",
    "The standard model predicts the next token given the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_logits(models, prompt, tokenizer, top_k=5):\n",
    "    \"\"\"Analyze top logits for each model to understand prediction quality.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'backbone'):  # MEDUSA model\n",
    "                outputs = model.backbone(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "            elif hasattr(model, 'num_tokens_to_predict'):  # Multi-token model\n",
    "                outputs = model(input_ids)\n",
    "                # Use the first head (standard next-token prediction)\n",
    "                if isinstance(outputs, dict) and 'logits' in outputs:\n",
    "                    logits = outputs['logits']\n",
    "                    if len(logits.shape) == 4:  # [batch, heads, seq, vocab]\n",
    "                        logits = logits[:, 0, -1, :]  # First head, last position\n",
    "                    else:\n",
    "                        logits = logits[:, -1, :]\n",
    "                else:\n",
    "                    logits = outputs.logits[:, -1, :]\n",
    "            else:  # Standard model\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Get top-k tokens\n",
    "            top_logits, top_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            top_probs = torch.softmax(top_logits, dim=-1)\n",
    "            \n",
    "            # Get token strings\n",
    "            top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices[0]]\n",
    "            \n",
    "            # Calculate entropy (measure of uncertainty)\n",
    "            all_probs = torch.softmax(logits, dim=-1)\n",
    "            entropy = -torch.sum(all_probs * torch.log(all_probs + 1e-10), dim=-1)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'tokens': top_tokens,\n",
    "                'probs': top_probs[0].tolist(),\n",
    "                'entropy': entropy.item(),\n",
    "                'confidence': top_probs[0][0].item()  # Confidence in top prediction\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_generation_quality(models, prompts, tokenizer, max_length=50):\n",
    "    \"\"\"Compare generation quality across models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print('='*60)\n",
    "        \n",
    "        results[prompt] = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            model.eval()\n",
    "            try:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(model, 'backbone'):\n",
    "                        generated = model.backbone.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                        \n",
    "                    elif hasattr(model, 'num_tokens_to_predict'):  # Mtp\n",
    "                        generated = model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            eos_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                        \n",
    "                    else:  #ntp\n",
    "                        generated = model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            repetition_penalty=1.1  # Reduce repetition\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                \n",
    "                results[prompt][model_name] = generated_text\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"'{generated_text}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error: {str(e)[:100]}...\"\n",
    "                results[prompt][model_name] = error_msg\n",
    "                print(f\"\\n{model_name}: {error_msg}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_standard_model import train_standard_gpt2\n",
    "\n",
    "standard_output_dir = \"standard_gpt2_outputs\"\n",
    "num_train_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "print(\"Training standard next-token prediction model...\")\n",
    "standard_model = train_standard_gpt2(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=standard_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-Token Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_modified_model import train_multi_token_gpt2, MultiTokenGPT2\n",
    "\n",
    "multi_token_output_dir = \"multi_gpt2_outputs\"\n",
    "num_tokens_to_predict = 4\n",
    "#trunk_layers = 12 - (num_tokens_to_predict - 1) = 9\n",
    "\n",
    "multi_token_model = train_multi_token_gpt2(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=multi_token_output_dir,\n",
    "    num_tokens_to_predict=num_tokens_to_predict,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# Explain the architecture\n",
    "trunk_layers = 12 - (num_tokens_to_predict - 1)  # = 9 for num_tokens_to_predict = 4\n",
    "print(f\"- Base model: GPT-2\")\n",
    "print(f\"- Trunk layers: {trunk_layers} (shared processing)\")\n",
    "print(f\"- Prediction heads: {num_tokens_to_predict} (first_head + {num_tokens_to_predict-1} extra_heads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 MEDUSA Model\n",
    "\n",
    "MEDUSA uses the standard model as its backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa import train_medusa, MedusaModel, generate_text_with_medusa\n",
    "\n",
    "medusa_output_dir = \"medusa_outputs\"\n",
    "num_medusa_heads = 5  # MEDUSA typically uses 5 heads\n",
    "\n",
    "# Use either a pretrained model or our standard model as the base\n",
    "base_model = standard_output_dir  # Path to our trained standard model\n",
    "\n",
    "print(\"Training MEDUSA model...\")\n",
    "medusa_model = train_medusa(\n",
    "    base_model=base_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    output_dir=medusa_output_dir,\n",
    "    num_medusa_heads=num_medusa_heads,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    freeze_backbone=True  # MEDUSA-1: freeze backbone for efficient training\n",
    ")\n",
    "\n",
    "# Explain the architecture\n",
    "print(f\"MEDUSA Model Architecture:\")\n",
    "print(f\"- Base model: Pretrained GPT-2\")\n",
    "print(f\"- Number of MEDUSA heads: {num_medusa_heads}\")\n",
    "print(f\"- Each head is a feed-forward network with SiLU activation and residual connection\")\n",
    "print(f\"- Tree verification allows multiple candidate predictions to be verified in parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Analysis and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_logits(models, prompt, tokenizer, top_k=5):\n",
    "    \"\"\"Analyze top logits for each model to understand prediction quality.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'backbone'):  #medusa\n",
    "                outputs = model.backbone(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "            elif hasattr(model, 'num_tokens_to_predict'):  #mtp\n",
    "                outputs = model(input_ids)\n",
    "                if isinstance(outputs, dict) and 'logits' in outputs:\n",
    "                    logits = outputs['logits']\n",
    "                    if len(logits.shape) == 4:\n",
    "                        logits = logits[:, 0, -1, :]\n",
    "                    else:\n",
    "                        logits = logits[:, -1, :]\n",
    "                else:\n",
    "                    logits = outputs.logits[:, -1, :]\n",
    "            else:\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            #top-k tokens\n",
    "            top_logits, top_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "            \n",
    "            #probabilities\n",
    "            top_probs = torch.softmax(top_logits, dim=-1)\n",
    "            \n",
    "            #token strings\n",
    "            top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices[0]]\n",
    "            \n",
    "            #entropy\n",
    "            all_probs = torch.softmax(logits, dim=-1)\n",
    "            entropy = -torch.sum(all_probs * torch.log(all_probs + 1e-10), dim=-1)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'tokens': top_tokens,\n",
    "                'probs': top_probs[0].tolist(),\n",
    "                'entropy': entropy.item(),\n",
    "                'confidence': top_probs[0][0].item()\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_generation_quality(models, prompts, tokenizer, max_length=50):\n",
    "    \"\"\"Compare generation quality across models - FIXED INDENTATION.\"\"\"\n",
    "    results = {}\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print('='*60)\n",
    "        results[prompt] = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            model.eval()\n",
    "            try:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(model, 'backbone'):  # MEDUSA\n",
    "                        generated = model.backbone.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "                    elif hasattr(model, 'num_tokens_to_predict'):  # Multi-token\n",
    "                        generated = model.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, use_speculative=True)\n",
    "                    else:  # Standard\n",
    "                        generated = model.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, pad_token_id=tokenizer.pad_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\n",
    "                \n",
    "                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                results[prompt][model_name] = generated_text\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"'{generated_text}'\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error: {str(e)[:100]}...\"\n",
    "                results[prompt][model_name] = error_msg\n",
    "                print(f\"\\n{model_name}: {error_msg}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_quality(models, prompts, tokenizer, max_length=50):\n",
    "    \"\"\"Compare generation quality across models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print('='*60)\n",
    "        \n",
    "        results[prompt] = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            model.eval()\n",
    "            try:\n",
    "                #tokenization with attention mask\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(model, 'backbone'):\n",
    "                        generated = model.backbone.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                        \n",
    "                    elif hasattr(model, 'num_tokens_to_predict'):  # Multi-token model\n",
    "                        # MultiTokenGPT2.generate() only accepts these parameters:\n",
    "                        # input_ids, attention_mask, max_new_tokens, temperature, top_k, top_p, do_sample, use_speculative\n",
    "                        generated = model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            use_speculative=True  #speculative decoding\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                        \n",
    "                    else:  # Standard model\n",
    "                        # Standard generation with all parameters\n",
    "                        generated = model.generate(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            max_new_tokens=max_length,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            repetition_penalty=1.1\n",
    "                        )\n",
    "                        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                \n",
    "                results[prompt][model_name] = generated_text\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"'{generated_text}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error: {str(e)[:100]}...\"\n",
    "                results[prompt][model_name] = error_msg\n",
    "                print(f\"\\n{model_name}: {error_msg}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_quality(models, prompts, tokenizer, max_length=50):\n",
    "    results = {}\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print('='*60)\n",
    "        results[prompt] = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            model.eval()\n",
    "            try:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(model, 'backbone'):  # MEDUSA\n",
    "                        generated = model.backbone.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "                    elif hasattr(model, 'num_tokens_to_predict'):  # Multi-token\n",
    "                        generated = model.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, use_speculative=True)\n",
    "                    else:  # Standard\n",
    "                        generated = model.generate(\n",
    "                            input_ids, attention_mask=attention_mask, max_new_tokens=max_length,\n",
    "                            do_sample=False, pad_token_id=tokenizer.pad_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)\n",
    "                \n",
    "                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                results[prompt][model_name] = generated_text\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"'{generated_text}'\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error: {str(e)[:100]}...\"\n",
    "                results[prompt][model_name] = error_msg\n",
    "                print(f\"\\n{model_name}: {error_msg}\")\n",
    "    return results\n",
    "\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Standard NTP\": standard_model,\n",
    "    \"Multi-Token Prediction\": multi_token_model, \n",
    "    \"MEDUSA\": medusa_model\n",
    "}\n",
    "\n",
    "print(\"=== LOGITS ANALYSIS ===\")\n",
    "test_prompt = \"The Pythagorean theorem states that\"\n",
    "logits_analysis = analyze_model_logits(models, test_prompt, tokenizer)\n",
    "\n",
    "for model_name, result in logits_analysis.items():\n",
    "    print(f\"\\n{model_name} Top 5 Predictions:\")\n",
    "    print(f\"Entropy: {result['entropy']:.3f} | Confidence: {result['confidence']:.3f}\")\n",
    "    for i, (token, prob) in enumerate(zip(result['tokens'], result['probs'])):\n",
    "        print(f\"  {i+1}. '{token}' (Probability: {prob:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n=== GENERATION QUALITY COMPARISON ===\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"The area of a triangle with sides 3, 4, and 5 is\",\n",
    "    \"To solve x^2 + 5x + 6 = 0, we\",\n",
    "    \"The derivative of f(x) = x^3 is\"\n",
    "]\n",
    "\n",
    "generation_results = compare_generation_quality(models, test_prompts, tokenizer, max_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation and Comparison\n",
    "\n",
    "Now, we'll evaluate and compare the three models in terms of:\n",
    "1. Generation quality\n",
    "2. Inference speed\n",
    "3. Acceptance rate for multi-token models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generation Quality Evaluation\n",
    "\n",
    "We'll use perplexity as a metric to evaluate the language modeling quality of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, dataloader):\n",
    "    \"\"\"Calculate perplexity of a model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n",
    "            #inputs to only include what the model expects\n",
    "            inputs = {}\n",
    "            for k in [\"input_ids\", \"attention_mask\", \"labels\"]:\n",
    "                if k in batch:\n",
    "                    inputs[k] = batch[k].to(device)\n",
    "            \n",
    "            #copy of the labels for loss calculation\n",
    "            labels = inputs[\"labels\"]\n",
    "            \n",
    "            #handling for different model types\n",
    "            if isinstance(model, MultiTokenGPT2):\n",
    "                #MultiTokenGPT2, manually compute loss using the first head\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                #logits and compute loss manually\n",
    "                logits = outputs[\"logits\"]\n",
    "                \n",
    "                if logits is not None:\n",
    "                    #the first head for perplexity calculation\n",
    "                    if len(logits.shape) == 4: \n",
    "                        logits = logits[:, 0] \n",
    "                    \n",
    "                    # Shift logits and labels for loss calculation\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()\n",
    "                    shift_labels = labels[..., 1:].contiguous()\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"sum\")\n",
    "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                                   shift_labels.view(-1))\n",
    "                    \n",
    "                    # Count non-padding tokens\n",
    "                    non_pad_tokens = (shift_labels != -100).sum().item()\n",
    "                    \n",
    "                    # Only add to totals if we got valid results\n",
    "                    if non_pad_tokens > 0:\n",
    "                        total_loss += loss.item()\n",
    "                        total_tokens += non_pad_tokens\n",
    "            elif isinstance(model, MedusaModel):\n",
    "                #use only the backbone for perplexity calculation\n",
    "                outputs = model.backbone(**inputs)\n",
    "                \n",
    "                if hasattr(outputs, \"loss\") and outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    #non-padding tokens\n",
    "                    non_pad_tokens = (labels != -100).sum().item()\n",
    "                    \n",
    "                    #add to totals if we got valid results\n",
    "                    if non_pad_tokens > 0:\n",
    "                        total_loss += loss.item() * non_pad_tokens\n",
    "                        total_tokens += non_pad_tokens\n",
    "            else:  # Standard model\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                if hasattr(outputs, \"loss\") and outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    non_pad_tokens = (labels != -100).sum().item()\n",
    "                    \n",
    "                    if non_pad_tokens > 0:\n",
    "                        total_loss += loss.item() * non_pad_tokens\n",
    "                        total_tokens += non_pad_tokens\n",
    "    \n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")  #if no tokens were processed\n",
    "        \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "print(\"Calculating perplexity on validation set...\")\n",
    "standard_ppl = calculate_perplexity(standard_model, val_dataloader)\n",
    "mtp_ppl = calculate_perplexity(multi_token_model, val_dataloader)\n",
    "medusa_ppl = calculate_perplexity(medusa_model, val_dataloader)\n",
    "\n",
    "print(f\"Standard model perplexity: {standard_ppl:.2f}\")\n",
    "print(f\"Multi-token model perplexity: {mtp_ppl:.2f}\")\n",
    "print(f\"MEDUSA model perplexity: {medusa_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference Speed Comparison\n",
    "\n",
    "Now, let's compare the inference speed of the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa import generate_text_with_medusa\n",
    "import time\n",
    "\n",
    "def measure_inference_speed(model_name, model, tokenizer, prompt, max_new_tokens=100, num_runs=5, \n",
    "                           use_multi_token=False, use_medusa=False, tree_branching=(5,5,3,3,2)):\n",
    "    \"\"\"Measure inference speed of a model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Set pad_token_id to eos_token_id if not set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"Warming up {model_name}...\")\n",
    "    if use_medusa:\n",
    "        output_ids = generate_text_with_medusa(model, tokenizer, prompt, max_new_tokens=20, tree_branching=tree_branching)\n",
    "        _ = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, \"num_tokens_to_predict\"):  # MultiTokenGPT2 model\n",
    "                generate_kwargs = {\n",
    "                    \"max_new_tokens\": 20,\n",
    "                    \"do_sample\": False,\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "                }\n",
    "                \n",
    "                if use_multi_token:\n",
    "                    generate_kwargs[\"use_speculative\"] = True\n",
    "            else:  # Standard model\n",
    "                generate_kwargs = {\n",
    "                    \"max_new_tokens\": 20,\n",
    "                    \"do_sample\": False,\n",
    "                    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "                }\n",
    "                \n",
    "            _ = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                **generate_kwargs\n",
    "            )\n",
    "    \n",
    "    # Measure time\n",
    "    times = []\n",
    "    tokens_generated = []\n",
    "    \n",
    "    print(f\"Running inference on {model_name}...\")\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if use_medusa:\n",
    "            # Generate tokens using Medusa\n",
    "            output_ids = generate_text_with_medusa(model, tokenizer, prompt, max_new_tokens=max_new_tokens, tree_branching=tree_branching)\n",
    "            num_new_tokens = len(output_ids[0]) - len(inputs[\"input_ids\"][0])\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #model-specific generate kwargs\n",
    "                if hasattr(model, \"num_tokens_to_predict\"):\n",
    "                    generate_kwargs = {\n",
    "                        \"max_new_tokens\": max_new_tokens,\n",
    "                        \"do_sample\": False,\n",
    "                        \"attention_mask\": inputs[\"attention_mask\"]\n",
    "                    }\n",
    "                    \n",
    "                    if use_multi_token:\n",
    "                        generate_kwargs[\"use_speculative\"] = True\n",
    "                else:\n",
    "                    generate_kwargs = {\n",
    "                        \"max_new_tokens\": max_new_tokens,\n",
    "                        \"do_sample\": False,\n",
    "                        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                        \"attention_mask\": inputs[\"attention_mask\"]\n",
    "                    }\n",
    "                    \n",
    "                output_ids = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    **generate_kwargs\n",
    "                )\n",
    "            num_new_tokens = output_ids.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "            # Decode for display (optional)\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        times.append(elapsed_time)\n",
    "        tokens_generated.append(num_new_tokens)\n",
    "        \n",
    "        print(f\"Run {i+1}: Generated {num_new_tokens} tokens in {elapsed_time:.2f}s ({num_new_tokens/elapsed_time:.2f} tokens/s)\")\n",
    "    \n",
    "    # Calculate average\n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_tokens = sum(tokens_generated) / len(tokens_generated)\n",
    "    tokens_per_second = avg_tokens / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"avg_tokens\": avg_tokens,\n",
    "        \"tokens_per_second\": tokens_per_second\n",
    "    }\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the derivative of f(x) = x^3 + 2x^2 - 5x + 7?\",\n",
    "    \"Solve the equation: 3x^2 - 12 = 0\",\n",
    "    \"If a triangle has sides of length 3, 4, and 5, what is its area?\"\n",
    "]\n",
    "\n",
    "#inference speed tests\n",
    "results = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nTesting prompt: {prompt}\")\n",
    "    \n",
    "    standard_result = measure_inference_speed(\"Standard NTP\", standard_model, tokenizer, prompt)\n",
    "    results.append(standard_result)\n",
    "    \n",
    "    mtp_result = measure_inference_speed(\"Multi-Token Prediction\", multi_token_model, tokenizer, prompt, use_multi_token=True)\n",
    "    results.append(mtp_result)\n",
    "    \n",
    "    medusa_result = measure_inference_speed(\"MEDUSA\", medusa_model, tokenizer, prompt, use_medusa=True)\n",
    "    results.append(medusa_result)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "speed_df = pd.DataFrame(results)\n",
    "print(\"\\nInference Speed Summary:\")\n",
    "print(speed_df.groupby('model').mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_speeds = speed_df.groupby('model')['tokens_per_second'].mean()\n",
    "avg_speeds.plot(kind='bar')\n",
    "plt.title('Average Inference Speed Comparison')\n",
    "plt.ylabel('Tokens per Second')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(avg_speeds):\n",
    "    plt.text(i, v + 0.5, f\"{v:.1f}\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
